{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "567ddc59",
   "metadata": {},
   "source": [
    "https://discuss.huggingface.co/t/adding-features-to-a-pretrained-language-model/770/4\n",
    "https://colab.research.google.com/drive/1eB8EMCwEE1_o5QOdC0gEejxqgkv6Q_cO?usp=sharing#scrollTo=QRwqVzOk0y7x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0127ddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# If a GPU is available\n",
    "if torch.cuda.is_available():    \n",
    "    #set device to GPU   \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If no GPU is available\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e4bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "\n",
    "import re\n",
    "import scipy\n",
    "import pandas         as pd\n",
    "import io\n",
    "import numpy          as np\n",
    "import copy\n",
    "import seaborn        as sns\n",
    "\n",
    "import transformers\n",
    "from transformers                     import  RobertaModel, RobertaTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.metrics                  import classification_report\n",
    "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from torch                            import nn, optim\n",
    "from torch.utils                      import data\n",
    "from sklearn.decomposition            import PCA\n",
    "\n",
    "#Seeding for deterministic results\n",
    "RANDOM_SEED = 64\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   torch.cuda.manual_seed(RANDOM_SEED)\n",
    "   torch.cuda.manual_seed_all(RANDOM_SEED) \n",
    "   torch.backends.cudnn.deterministic = True  \n",
    "   torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "CLASS_NAMES = ['no-fake', 'fake']\n",
    "MAX_LENGTH = 200\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 6\n",
    "HIDDEN_UNITS = 128\n",
    "\n",
    "tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-large')  #Use roberta-large or roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "952c8801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>economy</th>\n",
       "      <th>health-care</th>\n",
       "      <th>taxes</th>\n",
       "      <th>federal-budget</th>\n",
       "      <th>education</th>\n",
       "      <th>jobs</th>\n",
       "      <th>state-budget</th>\n",
       "      <th>candidates-biography</th>\n",
       "      <th>...</th>\n",
       "      <th>state_info_Virginia</th>\n",
       "      <th>state_info_Washington, D.C.</th>\n",
       "      <th>state_info_Wisconsin</th>\n",
       "      <th>state_info_other</th>\n",
       "      <th>party_affiliation_democrat</th>\n",
       "      <th>party_affiliation_independent</th>\n",
       "      <th>party_affiliation_none</th>\n",
       "      <th>party_affiliation_organization</th>\n",
       "      <th>party_affiliation_other</th>\n",
       "      <th>party_affiliation_republican</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>China is in the South China Sea and (building)...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>With the resources it takes to execute just ov...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>The (Wisconsin) governor has proposed tax give...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Says her representation of an ex-boyfriend who...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>At protests in Wisconsin against proposed coll...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                          statement  economy  \\\n",
       "0      1  China is in the South China Sea and (building)...        0   \n",
       "1      0  With the resources it takes to execute just ov...        0   \n",
       "2      0  The (Wisconsin) governor has proposed tax give...        0   \n",
       "3      1  Says her representation of an ex-boyfriend who...        0   \n",
       "4      0  At protests in Wisconsin against proposed coll...        0   \n",
       "\n",
       "   health-care  taxes  federal-budget  education  jobs  state-budget  \\\n",
       "0            0      0               0          0     0             0   \n",
       "1            1      0               0          0     0             0   \n",
       "2            0      1               0          0     0             0   \n",
       "3            0      0               0          0     0             0   \n",
       "4            1      0               0          0     0             1   \n",
       "\n",
       "   candidates-biography  ...  state_info_Virginia  \\\n",
       "0                     0  ...                    0   \n",
       "1                     0  ...                    0   \n",
       "2                     0  ...                    0   \n",
       "3                     1  ...                    0   \n",
       "4                     0  ...                    0   \n",
       "\n",
       "   state_info_Washington, D.C.  state_info_Wisconsin  state_info_other  \\\n",
       "0                            0                     0                 0   \n",
       "1                            0                     0                 1   \n",
       "2                            1                     0                 0   \n",
       "3                            0                     0                 0   \n",
       "4                            0                     1                 0   \n",
       "\n",
       "   party_affiliation_democrat  party_affiliation_independent  \\\n",
       "0                           0                              0   \n",
       "1                           1                              0   \n",
       "2                           1                              0   \n",
       "3                           0                              0   \n",
       "4                           0                              0   \n",
       "\n",
       "   party_affiliation_none  party_affiliation_organization  \\\n",
       "0                       0                               0   \n",
       "1                       0                               0   \n",
       "2                       0                               0   \n",
       "3                       1                               0   \n",
       "4                       0                               0   \n",
       "\n",
       "   party_affiliation_other  party_affiliation_republican  \n",
       "0                        0                             1  \n",
       "1                        0                             0  \n",
       "2                        0                             0  \n",
       "3                        0                             0  \n",
       "4                        0                             1  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('preprocessed.csv')\n",
    "train['label'] = train.label.astype(int)\n",
    "train.drop(['subject','speaker','id'], axis=1, inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc3268ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(train, test_size=0.2, random_state=RANDOM_SEED)\n",
    "X_train, X_dev = train_test_split(X_train, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ab43473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = torch.tensor(X_train.values).float()\n",
    "# x_dev = torch.tensor(X_dev.values).float()\n",
    "# x_test = torch.tensor(X_test.values).float()\n",
    "# #Converting prections for train, dev and test data to tensors\n",
    "# y_train = torch.tensor(y_train)\n",
    "# y_dev   = torch.tensor(y_dev)\n",
    "# y_test  = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efba790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a dataset which will be used to feed to RoBERTa\n",
    "class FakeNewDataset(data.Dataset):\n",
    "\n",
    "  def __init__(self, firstSeq, labelValue, extra_feats, tokenizer, max_len):\n",
    "    self.firstSeq    = firstSeq      #First input sequence that will be supplied to RoBERTa\n",
    "    self.labelValue  = labelValue    #label value for each training example in the dataset\n",
    "    self.tokenizer   = tokenizer     #tokenizer that will be used to tokenize input sequences (Uses BERT-tokenizer here)\n",
    "    self.max_len     = max_len       #Maximum length of the tokens from the input sequence that BERT needs to attend to\n",
    "    self.extra_feats = extra_feats\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.labelValue)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    firstSeq    = str(self.firstSeq[item])\n",
    "    \n",
    "    \n",
    "    #Encoding the first and the second sequence to a form accepted by RoBERTa\n",
    "    #RoBERTa does not use token_type_ids to distinguish the first sequence from the second sequnece.\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        firstSeq,\n",
    "        max_length = self.max_len,\n",
    "        add_special_tokens= True,\n",
    "        truncation = True,\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True,\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'firstSeq' : firstSeq,\n",
    "        'input_ids': encoding['input_ids'].flatten(),\n",
    "        'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        'labelValue'  : torch.tensor(self.labelValue[item], dtype=torch.long),\n",
    "        'extra_features' : torch.tensor(self.extra_feats[item]).float()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21c5b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a data loader\n",
    "def createDataLoader(dataframe, tokenizer, max_len, batch_size):\n",
    "  ds = FakeNewDataset(\n",
    "      firstSeq    = dataframe.statement.to_numpy(),\n",
    "      labelValue  = dataframe.label.to_numpy(),\n",
    "      extra_feats = dataframe.drop(['statement','label'],axis=1).to_numpy(),\n",
    "      tokenizer   = tokenizer,\n",
    "      max_len     = max_len\n",
    "  )\n",
    "\n",
    "  return data.DataLoader(\n",
    "      ds,\n",
    "      batch_size  = batch_size,\n",
    "      shuffle     = True,\n",
    "      num_workers = 4\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c93bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data loader for training data\n",
    "trainDataLoader        = createDataLoader(X_train, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
    "\n",
    "#Creating data loader for development data\n",
    "developmentDataLoader  = createDataLoader(X_dev, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
    "\n",
    "#Creating data loader for test data\n",
    "testDataLoader         = createDataLoader(X_test, tokenizer, MAX_LENGTH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f9d4572",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This class defines the model that will be used for \n",
    "training and testing on the dataset.\n",
    "\n",
    "Adapted from huggingFace\n",
    "This RoBERTa model from huggingface outputs the last hidden states\n",
    "and the pooled output by default. Pooled output is the classification \n",
    "token (1st token of the last hidden state) further processed by a Linear\n",
    "layer and a Tanh activation function.\n",
    "\n",
    "The pre-trained RoBERTa model is used as the primary model.\n",
    "This class experiments with RoBERTa and its ensemble with TF-IDF features. \n",
    "roberta-only :            No ensembling. This just fine-tunes the RoBERTa model. \n",
    "                          The pooled output is passed through a linear layer and \n",
    "                          softmax function is finally used for preictions. \n",
    "\n",
    "roberta-tfIdf :           This model conatenates the 1st token of last-hidden layer\n",
    "                          from RoBERTa with TF-IDF features. Various ways of this \n",
    "                          concatenation was experimented (using pooled output instead\n",
    "                          of 1st token of last hidden layer etc)\n",
    "\n",
    "roberta-pcaTfidf :        This model concatenates the pooled output from\n",
    "                          RoBERTa with the PCA transformed vector.\n",
    "\n",
    "roberta-preTrainedTfIdf : This model concatenates the pooled output from\n",
    "                          RoBERTa with the hidden layer output from a pre-trained\n",
    "                          SNN that was trained on TF-IDF features.\n",
    "\n",
    "Used dropout to prevent over-fitting.'''\n",
    "\n",
    "class FakeNewsClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self,  n_classes):\n",
    "    super(FakeNewsClassifier, self).__init__()\n",
    "    self.robertaModel              = RobertaModel.from_pretrained('roberta-large')    #use roberta-large or roberta-base\n",
    "\n",
    "    self.drop                      = nn.Dropout(p = 0.3)\n",
    "\n",
    "    self.output                    = nn.Linear(self.robertaModel.config.hidden_size, n_classes)\n",
    "\n",
    "    self.input_size_extrafeats     = self.robertaModel.config.hidden_size + len(X_train.drop(['statement','label'],axis=1).columns)\n",
    "    \n",
    "    self.dense                     = nn.Linear( self.input_size_extrafeats,  self.input_size_extrafeats)\n",
    "    self.out_proj                  = nn.Linear( self.input_size_extrafeats, n_classes)\n",
    "\n",
    "    \n",
    "    self.softmax                   = nn.Softmax(dim = 1)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, extrafeats, modelType):\n",
    "    \n",
    "    roberta_output     = self.robertaModel(\n",
    "        input_ids      = input_ids,               #Input sequence tokens\n",
    "        attention_mask = attention_mask )         #Mask to avoid performing attention on padding tokens\n",
    "    #print(roberta_output[1].shape)\n",
    "\n",
    "    if modelType   == 'roberta-only':\n",
    "      pooled_output = roberta_output[1]           #Using pooled output\n",
    "      output        = self.drop(pooled_output)\n",
    "      output        = self.output(output)\n",
    "\n",
    "    elif modelType == 'roberta-extra':\n",
    "      soutput = roberta_output[1]#---------        experimenting with pooled output \n",
    "      #soutput = roberta_output[0][:, 0, :]        #taking <s> token (equivalent to [CLS] token in BERT)\n",
    "      x       = torch.cat((soutput, extrafeats) , dim=1)\n",
    "      x       = self.drop(x)\n",
    "      output  = self.out_proj(x)\n",
    "\n",
    "    \n",
    "    return self.softmax(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f26928dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating a StanceClassifier object as our model and loading the model onto the GPU.\n",
    "model = FakeNewsClassifier(len(CLASS_NAMES))\n",
    "model = model.to(device)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e90e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Using the same optimiser as used in BERT paper\n",
    "with a different learning rate'''\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                  lr = 2e-6, \n",
    "                  correct_bias= False)\n",
    "\n",
    "totalSteps = len(trainDataLoader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps = totalSteps\n",
    ")\n",
    "\n",
    "'''Using class-weights to accomodate heavily imbalanced data. \n",
    "These weights were learnt by running several experiments using \n",
    "other weights and the weights that produced the best results have\n",
    "finally been used here'''\n",
    "\n",
    "weights      = [1.0, 1.0]\n",
    "classWeights = torch.FloatTensor(weights)\n",
    "lossFunction = nn.CrossEntropyLoss(weight = classWeights).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2032119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used for training the model. \n",
    "def train_epoch(\n",
    "  model,\n",
    "  dataLoader,\n",
    "  lossFunction,\n",
    "  optimizer,\n",
    "  device,\n",
    "  scheduler,\n",
    "  n_examples\n",
    "):\n",
    "\n",
    "  model = model.train()\n",
    "  losses = []\n",
    "  correctPredictions = 0\n",
    "\n",
    "  for d in tqdm(dataLoader):\n",
    "    \n",
    "    input_ids              = d[\"input_ids\"].to(device)                           #Loading input ids to GPU\n",
    "    attention_mask         = d[\"attention_mask\"].to(device)                      #Loading attention mask to GPU\n",
    "    labelValues            = d[\"labelValue\"].to(device)                          #Loading label value to GPU\n",
    "    extra_feats            = d[\"extra_features\"]                                    \n",
    "    extra_feats = extra_feats.to(device)\n",
    "    \n",
    "\n",
    "    #Getting the output from our model (Object of StanceClassification class) for train data\n",
    "    outputs = model(\n",
    "      input_ids             = input_ids,\n",
    "      attention_mask        = attention_mask,\n",
    "      extrafeats    = extra_feats,\n",
    "      modelType             = 'roberta-extra'\n",
    "    )\n",
    "\n",
    "    #Determining the model predictions\n",
    "    _, predictionIndices = torch.max(outputs, dim=1)\n",
    "    loss = lossFunction(outputs, labelValues)\n",
    "\n",
    "    #Calculating the correct predictions for accuracy\n",
    "    correctPredictions += torch.sum(predictionIndices == labelValues)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return np.mean(losses), correctPredictions.double() / n_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa4fde5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used for evaluating the model on the development and test set\n",
    "def eval_model(\n",
    "    model, \n",
    "    dataLoader, \n",
    "    lossFunction,\n",
    "    device,\n",
    "    n_examples,\n",
    "    dev = False\n",
    "    ):\n",
    "  \n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correctPredictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in tqdm(dataLoader):\n",
    "\n",
    "      input_ids              = d[\"input_ids\"].to(device)                          #Loading input ids to GPU\n",
    "      attention_mask         = d[\"attention_mask\"].to(device)                     #Loading attention mask to GPU\n",
    "      labelValues            = d[\"labelValue\"].to(device)                         #Loading label values to GPU\n",
    "      extra_feats            = d[\"extra_features\"]                                    \n",
    "      extra_feats = extra_feats.to(device)\n",
    "\n",
    "      #Getting the softmax output from model for dev data\n",
    "      outputs = model(\n",
    "        input_ids             = input_ids,\n",
    "        attention_mask        = attention_mask,\n",
    "        extrafeats    = extra_feats,\n",
    "        modelType             = 'roberta-extra'\n",
    "      )\n",
    "\n",
    "      #Determining the model predictions\n",
    "      _, predictionIndices = torch.max(outputs, dim=1)\n",
    "      loss = lossFunction(outputs, labelValues)\n",
    "\n",
    "      #Calculating the correct predictions for accuracy\n",
    "      correctPredictions += torch.sum(predictionIndices == labelValues)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return np.mean(losses), correctPredictions.double() / n_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e4c9284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/716 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 716/716 [05:06<00:00,  2.34it/s]\n",
      "  0%|          | 0/179 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.6378429572841975 Training accuracy 0.6466480446927374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 179/179 [00:22<00:00,  7.94it/s]\n",
      "  0%|          | 0/716 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development loss 0.618541777300435 Development accuracy 0.6585195530726258\n",
      "\n",
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 716/716 [05:10<00:00,  2.30it/s]\n",
      "  0%|          | 0/179 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.6133500358412386 Training accuracy 0.678945530726257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 179/179 [00:22<00:00,  7.90it/s]\n",
      "  0%|          | 0/716 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development loss 0.604751403271819 Development accuracy 0.6822625698324023\n",
      "\n",
      "\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 716/716 [05:10<00:00,  2.31it/s]\n",
      "  0%|          | 0/179 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.5939404240616873 Training accuracy 0.7068784916201117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 179/179 [00:22<00:00,  7.98it/s]\n",
      "  0%|          | 0/716 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development loss 0.6138293358533742 Development accuracy 0.6843575418994413\n",
      "\n",
      "\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 716/716 [05:09<00:00,  2.31it/s]\n",
      "  0%|          | 0/179 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.5818448495848219 Training accuracy 0.7245111731843575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 179/179 [00:22<00:00,  7.94it/s]\n",
      "  0%|          | 0/716 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development loss 0.6171875392924474 Development accuracy 0.6829608938547487\n",
      "\n",
      "\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 716/716 [05:10<00:00,  2.30it/s]\n",
      "  0%|          | 0/179 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.5660379976034164 Training accuracy 0.7381284916201117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 179/179 [00:22<00:00,  7.85it/s]\n",
      "  0%|          | 0/716 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development loss 0.6206617401964838 Development accuracy 0.6815642458100559\n",
      "\n",
      "\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 716/716 [05:09<00:00,  2.31it/s]\n",
      "  0%|          | 0/179 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.5520178599897043 Training accuracy 0.7552374301675978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 179/179 [00:22<00:00,  7.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development loss 0.622540844219357 Development accuracy 0.678072625698324\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#fine tuning ROBERTa and validating it \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  print(f'Epoch {epoch + 1}')\n",
    "  trainLoss, trainAccuracy = train_epoch(\n",
    "    model,\n",
    "    trainDataLoader,\n",
    "    lossFunction,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    len(X_train)\n",
    "  )\n",
    "  \n",
    "  print(f'Training loss {trainLoss} Training accuracy {trainAccuracy}')\n",
    "\n",
    "  devLoss, devAccuracy = eval_model(\n",
    "    model,\n",
    "    developmentDataLoader,\n",
    "    lossFunction,\n",
    "    device,\n",
    "    len(X_dev),\n",
    "    dev = True\n",
    "  )\n",
    "\n",
    "  print(f'Development loss {devLoss} Development accuracy {devAccuracy}')\n",
    "  print()\n",
    "  \n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71caaa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function gets the predictions from the model after it is trained.\n",
    "def get_predictions(model, data_loader):\n",
    "\n",
    "  model = model.eval()\n",
    "\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in tqdm(data_loader):\n",
    "\n",
    "\n",
    "      input_ids              = d[\"input_ids\"].to(device)\n",
    "      attention_mask         = d[\"attention_mask\"].to(device)\n",
    "      labels                 = d[\"labelValue\"].to(device)\n",
    "      extra_feats            = d[\"extra_features\"].to(device)                                \n",
    "\n",
    "      #Getting the softmax output from model\n",
    "      outputs = model(\n",
    "        input_ids             = input_ids,\n",
    "        attention_mask        = attention_mask,\n",
    "        extrafeats    = extra_feats,\n",
    "        modelType             = 'roberta-extra'\n",
    "      )\n",
    "\n",
    "      _, preds = torch.max(outputs, dim=1)     #Determining the model predictions\n",
    "\n",
    "\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(outputs)\n",
    "      real_values.extend(labels)\n",
    "\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "  \n",
    "  return predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "844debed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/179 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 179/179 [00:21<00:00,  8.28it/s]\n"
     ]
    }
   ],
   "source": [
    "#Getting model predictions on dev dataset\n",
    "yHat_dev, predProbs_dev, yTest_dev = get_predictions(\n",
    "  model,\n",
    "  developmentDataLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1193907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no-fake       0.55      0.44      0.49       501\n",
      "        fake       0.73      0.81      0.76       931\n",
      "\n",
      "    accuracy                           0.68      1432\n",
      "   macro avg       0.64      0.62      0.63      1432\n",
      "weighted avg       0.67      0.68      0.67      1432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  #Printing classification report for dev dataset (Evaluating the model on Dev set)\n",
    "print(classification_report(yTest_dev, yHat_dev, target_names= CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aeb5ad7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/224 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 224/224 [00:25<00:00,  8.77it/s]\n"
     ]
    }
   ],
   "source": [
    "#Getting model predictions on test dataset\n",
    "yHat_test, predProbs_test, yTest_test = get_predictions(\n",
    "  model,\n",
    "  testDataLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4f5a94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no-fake       0.54      0.44      0.48       629\n",
      "        fake       0.72      0.80      0.76      1161\n",
      "\n",
      "    accuracy                           0.67      1790\n",
      "   macro avg       0.63      0.62      0.62      1790\n",
      "weighted avg       0.66      0.67      0.66      1790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing classification report for test dataset (Evaluating the model on test set)\n",
    "print(classification_report(yTest_test, yHat_test, target_names= CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51c0625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './results/custom_transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63cd9dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>economy</th>\n",
       "      <th>health-care</th>\n",
       "      <th>taxes</th>\n",
       "      <th>federal-budget</th>\n",
       "      <th>education</th>\n",
       "      <th>jobs</th>\n",
       "      <th>...</th>\n",
       "      <th>state_info_Virginia</th>\n",
       "      <th>state_info_Washington, D.C.</th>\n",
       "      <th>state_info_Wisconsin</th>\n",
       "      <th>state_info_other</th>\n",
       "      <th>party_affiliation_democrat</th>\n",
       "      <th>party_affiliation_independent</th>\n",
       "      <th>party_affiliation_none</th>\n",
       "      <th>party_affiliation_organization</th>\n",
       "      <th>party_affiliation_other</th>\n",
       "      <th>party_affiliation_republican</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dc32e5ffa8b</td>\n",
       "      <td>Five members of [the Common Cause Georgia] boa...</td>\n",
       "      <td>campaign-finance,ethics,government-regulation</td>\n",
       "      <td>kasim-reed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa49bb41cab</td>\n",
       "      <td>Theres no negative advertising in my campaign ...</td>\n",
       "      <td>elections</td>\n",
       "      <td>bill-mccollum</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dddc8d12ac1</td>\n",
       "      <td>Leticia Van de Putte voted to give illegal imm...</td>\n",
       "      <td>health-care,immigration,public-health</td>\n",
       "      <td>dan-patrick</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bcfe8f51667</td>\n",
       "      <td>Fiorinas plan would mean slashing Social Secur...</td>\n",
       "      <td>federal-budget,medicare,social-security</td>\n",
       "      <td>barbara-boxer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eedbbaff5ab</td>\n",
       "      <td>By the end of his first term, President Obama ...</td>\n",
       "      <td>federal-budget,new-hampshire-2012</td>\n",
       "      <td>mitt-romney</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                          statement  \\\n",
       "0  dc32e5ffa8b  Five members of [the Common Cause Georgia] boa...   \n",
       "1  aa49bb41cab  Theres no negative advertising in my campaign ...   \n",
       "2  dddc8d12ac1  Leticia Van de Putte voted to give illegal imm...   \n",
       "3  bcfe8f51667  Fiorinas plan would mean slashing Social Secur...   \n",
       "4  eedbbaff5ab  By the end of his first term, President Obama ...   \n",
       "\n",
       "                                         subject        speaker  economy  \\\n",
       "0  campaign-finance,ethics,government-regulation     kasim-reed        0   \n",
       "1                                      elections  bill-mccollum        0   \n",
       "2          health-care,immigration,public-health    dan-patrick        0   \n",
       "3        federal-budget,medicare,social-security  barbara-boxer        0   \n",
       "4              federal-budget,new-hampshire-2012    mitt-romney        0   \n",
       "\n",
       "   health-care  taxes  federal-budget  education  jobs  ...  \\\n",
       "0            0      0               0          0     0  ...   \n",
       "1            0      0               0          0     0  ...   \n",
       "2            1      0               0          0     0  ...   \n",
       "3            0      0               1          0     0  ...   \n",
       "4            0      0               1          0     0  ...   \n",
       "\n",
       "   state_info_Virginia  state_info_Washington, D.C.  state_info_Wisconsin  \\\n",
       "0                    0                            0                     0   \n",
       "1                    0                            0                     0   \n",
       "2                    0                            0                     0   \n",
       "3                    0                            0                     0   \n",
       "4                    0                            0                     0   \n",
       "\n",
       "   state_info_other  party_affiliation_democrat  \\\n",
       "0                 0                           1   \n",
       "1                 0                           0   \n",
       "2                 0                           0   \n",
       "3                 0                           1   \n",
       "4                 0                           0   \n",
       "\n",
       "   party_affiliation_independent  party_affiliation_none  \\\n",
       "0                              0                       0   \n",
       "1                              0                       0   \n",
       "2                              0                       0   \n",
       "3                              0                       0   \n",
       "4                              0                       0   \n",
       "\n",
       "   party_affiliation_organization  party_affiliation_other  \\\n",
       "0                               0                        0   \n",
       "1                               0                        0   \n",
       "2                               0                        0   \n",
       "3                               0                        0   \n",
       "4                               0                        0   \n",
       "\n",
       "   party_affiliation_republican  \n",
       "0                             0  \n",
       "1                             1  \n",
       "2                             1  \n",
       "3                             0  \n",
       "4                             1  \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test_preprocessed.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c2e376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cadb6533",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = test_df.id.copy()\n",
    "test_df.drop(columns=['id','subject','speaker'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fb209dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data loader for test data\n",
    "testDataLoader_real         = createDataLoader(test_df, tokenizer, MAX_LENGTH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0b0669d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/480 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 480/480 [00:54<00:00,  8.87it/s]\n"
     ]
    }
   ],
   "source": [
    "#Getting model predictions on test dataset\n",
    "yHat_test, predProbs_test, yTest_test = get_predictions(\n",
    "  model,\n",
    "  testDataLoader_real\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "18fe9564",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.Series(yHat_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5e01d49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dc32e5ffa8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa49bb41cab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dddc8d12ac1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bcfe8f51667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eedbbaff5ab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3831</th>\n",
       "      <td>e050483b866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3832</th>\n",
       "      <td>6221e28aa63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3833</th>\n",
       "      <td>954dc0f0b5d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3834</th>\n",
       "      <td>2fa476b0d2f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3835</th>\n",
       "      <td>b7c0ba77bd3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3836 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id\n",
       "0     dc32e5ffa8b\n",
       "1     aa49bb41cab\n",
       "2     dddc8d12ac1\n",
       "3     bcfe8f51667\n",
       "4     eedbbaff5ab\n",
       "...           ...\n",
       "3831  e050483b866\n",
       "3832  6221e28aa63\n",
       "3833  954dc0f0b5d\n",
       "3834  2fa476b0d2f\n",
       "3835  b7c0ba77bd3\n",
       "\n",
       "[3836 rows x 1 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(ids)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a8542415",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['label']=label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "667773d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dc32e5ffa8b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa49bb41cab</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dddc8d12ac1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bcfe8f51667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eedbbaff5ab</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3831</th>\n",
       "      <td>e050483b866</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3832</th>\n",
       "      <td>6221e28aa63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3833</th>\n",
       "      <td>954dc0f0b5d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3834</th>\n",
       "      <td>2fa476b0d2f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3835</th>\n",
       "      <td>b7c0ba77bd3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3836 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  label\n",
       "0     dc32e5ffa8b      0\n",
       "1     aa49bb41cab      1\n",
       "2     dddc8d12ac1      1\n",
       "3     bcfe8f51667      1\n",
       "4     eedbbaff5ab      1\n",
       "...           ...    ...\n",
       "3831  e050483b866      1\n",
       "3832  6221e28aa63      1\n",
       "3833  954dc0f0b5d      1\n",
       "3834  2fa476b0d2f      0\n",
       "3835  b7c0ba77bd3      1\n",
       "\n",
       "[3836 rows x 2 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.to_csv('submission_custom_transformer.csv',index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152e904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
