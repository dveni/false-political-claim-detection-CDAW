{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "322a8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "\n",
    "import re\n",
    "import scipy\n",
    "import pandas         as pd\n",
    "import io\n",
    "import numpy          as np\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics                  import classification_report\n",
    "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch                            import nn, optim\n",
    "from torch.utils                      import data\n",
    "\n",
    "#Seeding for deterministic results\n",
    "RANDOM_SEED = 16\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "HIDDEN_LAYER_UNITS = 60\n",
    "\n",
    "CLASS_NAMES = ['no-fake', 'fake']\n",
    "EPOCHS      = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3882074",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('preprocessed.csv')\n",
    "train.drop(columns=['id', 'statement','subject','speaker'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4080445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>economy</th>\n",
       "      <th>health-care</th>\n",
       "      <th>taxes</th>\n",
       "      <th>federal-budget</th>\n",
       "      <th>education</th>\n",
       "      <th>jobs</th>\n",
       "      <th>state-budget</th>\n",
       "      <th>candidates-biography</th>\n",
       "      <th>elections</th>\n",
       "      <th>...</th>\n",
       "      <th>state_info_Virginia</th>\n",
       "      <th>state_info_Washington, D.C.</th>\n",
       "      <th>state_info_Wisconsin</th>\n",
       "      <th>state_info_other</th>\n",
       "      <th>party_affiliation_democrat</th>\n",
       "      <th>party_affiliation_independent</th>\n",
       "      <th>party_affiliation_none</th>\n",
       "      <th>party_affiliation_organization</th>\n",
       "      <th>party_affiliation_other</th>\n",
       "      <th>party_affiliation_republican</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  economy  health-care  taxes  federal-budget  education  jobs  \\\n",
       "0    1.0        0            0      0               0          0     0   \n",
       "1    0.0        0            1      0               0          0     0   \n",
       "2    0.0        0            0      1               0          0     0   \n",
       "3    1.0        0            0      0               0          0     0   \n",
       "4    0.0        0            1      0               0          0     0   \n",
       "\n",
       "   state-budget  candidates-biography  elections  ...  state_info_Virginia  \\\n",
       "0             0                     0          0  ...                    0   \n",
       "1             0                     0          0  ...                    0   \n",
       "2             0                     0          0  ...                    0   \n",
       "3             0                     1          0  ...                    0   \n",
       "4             1                     0          0  ...                    0   \n",
       "\n",
       "   state_info_Washington, D.C.  state_info_Wisconsin  state_info_other  \\\n",
       "0                            0                     0                 0   \n",
       "1                            0                     0                 1   \n",
       "2                            1                     0                 0   \n",
       "3                            0                     0                 0   \n",
       "4                            0                     1                 0   \n",
       "\n",
       "   party_affiliation_democrat  party_affiliation_independent  \\\n",
       "0                           0                              0   \n",
       "1                           1                              0   \n",
       "2                           1                              0   \n",
       "3                           0                              0   \n",
       "4                           0                              0   \n",
       "\n",
       "   party_affiliation_none  party_affiliation_organization  \\\n",
       "0                       0                               0   \n",
       "1                       0                               0   \n",
       "2                       0                               0   \n",
       "3                       1                               0   \n",
       "4                       0                               0   \n",
       "\n",
       "   party_affiliation_other  party_affiliation_republican  \n",
       "0                        0                             1  \n",
       "1                        0                             0  \n",
       "2                        0                             0  \n",
       "3                        0                             0  \n",
       "4                        0                             1  \n",
       "\n",
       "[5 rows x 120 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1ca2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train.drop('label', axis=1), train.label, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e039fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype(int)\n",
    "y_train = y_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f368e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cc31345",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train= y_train.tolist()\n",
    "y_test= y_test.tolist()\n",
    "y_dev= y_dev.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b04808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(X_train.values).float()\n",
    "x_dev = torch.tensor(X_dev.values).float()\n",
    "x_test = torch.tensor(X_test.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ef353ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting prections for train, dev and test data to tensors\n",
    "y_train = torch.tensor(y_train)\n",
    "y_dev   = torch.tensor(y_dev)\n",
    "y_test  = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d38695c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b6d7eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden  = nn.Linear(x_train.shape[1], HIDDEN_LAYER_UNITS)\n",
    "        # Output layer\n",
    "        self.output  =  nn.Linear(HIDDEN_LAYER_UNITS, len(CLASS_NAMES))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Defining tanh activation and softmax output \n",
    "        self.tanh    = nn.Tanh()                                     #Using tanh as it performed better than ReLu during hyper-param optimisation\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of the below operations\n",
    "        x = self.hidden(x)\n",
    "\n",
    "        #print(x.shape)\n",
    "        y = self.tanh(x)\n",
    "        #print(y.shape)\n",
    "        z = self.dropout(y)\n",
    "        #print(z.shape)\n",
    "        z = self.output(z)\n",
    "        #print(z.shape)\n",
    "        z = self.softmax(z)\n",
    "        \n",
    "        #returning the output from hidden layer and the output layer\n",
    "        return  y, z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e2e3355a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5226, 0.4774],\n",
      "        [0.5081, 0.4919],\n",
      "        [0.4573, 0.5427],\n",
      "        ...,\n",
      "        [0.4282, 0.5718],\n",
      "        [0.4289, 0.5711],\n",
      "        [0.4538, 0.5462]], grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "#Defining the model\n",
    "model = Nn()\n",
    "\n",
    "# Defining the loss\n",
    "'''Using class-weights to accomodate heavily imbalanced data. \n",
    "These weights were learnt by running several experiments using \n",
    "other weights and the weights that produced the best results have\n",
    " finally been used here'''\n",
    "\n",
    "weights       = [1.0, 1.0]\n",
    "class_weights = torch.FloatTensor(weights)\n",
    "criterion     = nn.CrossEntropyLoss(weight = class_weights)\n",
    "\n",
    "\n",
    "# Forward pass, get our logits\n",
    "hidden_state_output, classfier_output = model(x_train)\n",
    "print(classfier_output)\n",
    "print(classfier_output[0].shape)\n",
    "\n",
    "loss = criterion(classfier_output, y_train)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d8718ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10..  Training Loss: 0.648..  Dev Loss: 0.648..  Dev Accuracy: 0.654\n",
      "Epoch: 2/10..  Training Loss: 0.639..  Dev Loss: 0.639..  Dev Accuracy: 0.654\n",
      "Epoch: 3/10..  Training Loss: 0.639..  Dev Loss: 0.639..  Dev Accuracy: 0.654\n",
      "Epoch: 4/10..  Training Loss: 0.637..  Dev Loss: 0.637..  Dev Accuracy: 0.655\n",
      "Epoch: 5/10..  Training Loss: 0.633..  Dev Loss: 0.633..  Dev Accuracy: 0.654\n",
      "Epoch: 6/10..  Training Loss: 0.632..  Dev Loss: 0.632..  Dev Accuracy: 0.654\n",
      "Epoch: 7/10..  Training Loss: 0.636..  Dev Loss: 0.636..  Dev Accuracy: 0.631\n",
      "Epoch: 8/10..  Training Loss: 0.639..  Dev Loss: 0.639..  Dev Accuracy: 0.627\n",
      "Epoch: 9/10..  Training Loss: 0.636..  Dev Loss: 0.636..  Dev Accuracy: 0.632\n",
      "Epoch: 10/10..  Training Loss: 0.633..  Dev Loss: 0.633..  Dev Accuracy: 0.652\n"
     ]
    }
   ],
   "source": [
    "#Training the model on training data and evaluating it on development set\n",
    "#%%time\n",
    "def train_model():\n",
    "  train_losses = []\n",
    "  dev_losses = []\n",
    "  dev_accuracies = []\n",
    "\n",
    "  for e in range(EPOCHS):\n",
    "    correct_predictions = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    hidden_layer_output, classifier_output = model.forward(x_train)\n",
    "\n",
    "    loss = criterion(classifier_output, y_train)\n",
    "    loss.backward()\n",
    "    train_loss = loss.item()\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        #Getting hidden layer and softmax output from model for dev data\n",
    "        hidden_layer_output, classifier_output = model(x_dev)\n",
    "        \n",
    "        #Calculating loss\n",
    "        dev_loss = criterion(classifier_output, y_dev)\n",
    "        dev_losses.append(dev_loss)\n",
    "\n",
    "        #Calculating values predicted by the model\n",
    "        _, preds = torch.max(classifier_output, dim=1)\n",
    "        correct_predictions += torch.sum(preds == y_dev)\n",
    "        \n",
    "        #Calculating accuracy\n",
    "        dev_accuracy = correct_predictions.double() / len(y_dev)\n",
    "        dev_accuracies.append(dev_accuracy)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print(f\"Epoch: {e+1}/{EPOCHS}.. \",\n",
    "          f\"Training Loss: {dev_loss:.3f}.. \",\n",
    "          f\"Dev Loss: {dev_loss:.3f}.. \",\n",
    "          f\"Dev Accuracy: {dev_accuracy:.3f}\")\n",
    "\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bde225fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function gets the predictions for each data point \n",
    "in the deevelopment and the training set'''\n",
    "\n",
    "def get_predictions(model, x_test, y_test):\n",
    "\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    labels = y_test\n",
    "\n",
    "    #Currently, not interested in the hidden layer outputs.\n",
    "    _,classifier_output = model(x_test)\n",
    "\n",
    "    #Not interested in the maximum values, interested with the indices of these max values\n",
    "    _, preds = torch.max(classifier_output, dim=1)\n",
    "\n",
    "    predictions.extend(preds)\n",
    "    prediction_probs.extend(classifier_output)\n",
    "    real_values.extend(labels)\n",
    "  predictions = torch.stack(predictions)\n",
    "\n",
    "  prediction_probs = torch.stack(prediction_probs)\n",
    "  real_values = torch.stack(real_values)\n",
    "  return  predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6597cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predictions for the development set\n",
    "y_pred_dev, y_pred_probs, y_true_dev = get_predictions(\n",
    "  model,\n",
    "  x_dev, \n",
    "  y_dev\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c9fb7e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no-fake     0.4933    0.2238    0.3079       496\n",
      "        fake     0.6810    0.8782    0.7671       936\n",
      "\n",
      "    accuracy                         0.6515      1432\n",
      "   macro avg     0.5872    0.5510    0.5375      1432\n",
      "weighted avg     0.6160    0.6515    0.6081      1432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing the classifictaion report for the Development set\n",
    "print(classification_report(y_true_dev, y_pred_dev ,digits =4, target_names=CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c4c1aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the predictions for the test set\n",
    "y_pred_test, y_pred_probs, y_true_test = get_predictions(\n",
    "  model,\n",
    "  x_test, \n",
    "  y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e8bf13b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no-fake     0.5157    0.2256    0.3139       656\n",
      "        fake     0.6620    0.8774    0.7546      1134\n",
      "\n",
      "    accuracy                         0.6385      1790\n",
      "   macro avg     0.5888    0.5515    0.5343      1790\n",
      "weighted avg     0.6084    0.6385    0.5931      1790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true_test, y_pred_test , digits = 4,  target_names=CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971cc813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
